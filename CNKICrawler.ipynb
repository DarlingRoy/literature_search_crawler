{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNKI数据检索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照用户输入的检索词，从知网上获得对应的文献信息，信息包括题目、关键词、作者、机构、发表时间、下载次数、被引次数。并可以将这些信息用于后续的数据分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便爬取，我选择的入口地址是'http://search.cnki.com.cn/'。\n",
    "用户输入检索词并点击搜索后，会跳转到另外一个页面，我们再对跳转后得到的页面的URL进行分析。\n",
    "如输入检索词“人工智能”，得到的页面的URL为'http://search.cnki.com.cn/search.aspx?q=%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd&rank=relevant&cluster=all&val=&p=0'\n",
    "可以看出，其中的'q=%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd'被编码了，我们可以用urllib.parse.unquote()进行解码，发现解码后的URL为'http://search.cnki.com.cn/search.aspx?q=人工智能&rank=relevant&cluster=all&val=&p=0'。\n",
    "其中，q的值是用户输入的检索词；rank的值表明文献按哪种方式排序，默认按相关度，即'rank=relevant'，也可以按被引次数，为'rank=citeNumber'，也可以按下载次数，为'rank=download'，也可以按时间，为'date'；p的值用于控制翻页，p=0\n",
    "为第1页，每一页有15条记录，故第2页为p=15，第3页为p=30，以此类推。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在掌握了获取URL的方法后，下面我们开始对网页上的数据进行爬取。用列表存储获得的数据，用字典的形式存储一条文献的信息。但是由于被引次数、下载次数和关键词、作者这两组信息分别存储在两个不同的网页中，\n",
    "而题目、机构、发表时间在两个网页中都有，所以用两个列表，一个存储题目、机构、下载次数、被引次数，一个存储题目、关键词、机构、作者、发表时间，将两个列表转换为DataFrame形式，然后以题目和机构作为公共属性合并两个DataFrame。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面先写出代码的基本框架："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#获得页面上的HTML内容\n",
    "def getHTMLText(url):\n",
    "    return ''\n",
    "\n",
    "#获得搜索到的页面上的各篇论文的详细信息的入口地址，把所有的入口地址存储在列表中\n",
    "def getURLList(url,hrefList):\n",
    "    pass\n",
    "\n",
    "#获得论文的题目、作者、机构、发表时间、关键词\n",
    "def getPaperInfo(lst,hrefList):\n",
    "    pass\n",
    "\n",
    "#获得论文的题目、机构、下载次数、被引次数\n",
    "def getPaperInfo2(url,lst):\n",
    "    pass\n",
    "\n",
    "#将两个列表转换为DataFrame类型，并把两个列表连接起来\n",
    "def formatDataset(lst1,lst2):\n",
    "    pass\n",
    "\n",
    "#定义main函数调用其他的函数\n",
    "def main():\n",
    "    quaryWord=input(\"请输入检索词:\")\n",
    "    pageNum=2\n",
    "    paperInfo=[]    #用于存储论文的题目、作者、机构、发表时间、关键词\n",
    "    paperInfo2=[]   #用于存储论文的题目、机构、下载次数、被引次数\n",
    "    start_url='http://search.cnki.com.cn'+'/Search.aspx?q='+quote(quaryWord)+'&rank=relevant=&cluster=all&val='\n",
    "    for i in range(pageNum):\n",
    "        hrefs=[]\n",
    "        quary_url=start_url+'&p='+str(i*15)\n",
    "        getURLList(quary_url,hrefs)\n",
    "        getPaperInfo2(quary_url,paperInfo2)\n",
    "        getPaperInfo(paperInfo,hrefs)\n",
    "    formatDataset(paperInfo,paperInfo2)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在main函数中，pageNum表明从第一页开始一共需要爬取多少页，quote(quaryWord)用于对用户输入的检索词进行编码，quary_url为根据用户输入的检索词和页码生成的URL。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在getHTMLText函数中，我们可以用requests库获得html页面的源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r=requests.get(url)\n",
    "        r.raise_for_status\n",
    "        r.encoding='utf-8'\n",
    "        return r.text\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个例子，当用户输入“人工智能”时得到的页面的源码中，要找到其中第一篇论文的入口地址，需要找到的代码段为："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"wz_content\">\n",
    "  <h3>\n",
    "    <a href=\"http://www.cnki.com.cn/Article/CJFDTOTAL-KJDB201615009.htm\" target=\"_blank\">\n",
    "      中国\n",
    "      <span class=\"red_title\">\n",
    "        <span class=\"red_title\">\n",
    "          人工智能\n",
    "        </span>\n",
    "      </span>\n",
    "      40年\n",
    "    </a>\n",
    "    <a href=\"http://search.cnki.net/down/default.aspx?filename=KJDB201615009&amp;dbcode=CJFD&amp;year=2016&amp;dflag=pdfdown\" target=\"_blank\">\n",
    "      <img height=\"11\" src=\"images/search3-download-icon.jpg\" width=\"13\" />\n",
    "    </a>\n",
    "  </h3>\n",
    "  <div class=\"width715\">\n",
    "    <span class=\"text\">\n",
    "      介绍了中国\n",
    "      <span class=\"red_text\">\n",
    "        人工智能\n",
    "      </span>\n",
    "      从迷雾重重、艰难起步,到冲破禁锢、初露曙光,直至迎来春天、走上大道的发展历程。概括了中国\n",
    "      <span class=\"red_text\">\n",
    "        人工智能\n",
    "      </span>\n",
    "      取得的主要成绩和存在的问题,提出了发展中国\n",
    "      <span class=\"red_text\">\n",
    "        人工智能\n",
    "      </span>\n",
    "      的决策建议。\n",
    "    </span>\n",
    "  </div>\n",
    "  <span class=\"year-count\">\n",
    "    <span title=\"科技导报\">\n",
    "      《科技导报》 2016年 第15期\n",
    "    </span>\n",
    "    <span class=\"count\">\n",
    "      下载次数（8196）| 被引次数（66）\n",
    "    </span>\n",
    "  </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于其余的14篇论文，其入口也是在这个相应的代码段中。在上面代码中可以看出，要找到入口地址，要先找到具有{'class':'wz_content'}属性的div标签，再在其中找到第一个具有{'target':_blank'}属性的a标签，获取其中的href属性的值。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getURLList(url,hrefList):\n",
    "    html=getHTMLText(url)\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    for div in soup.find_all('div',attrs={'class':'wz_content'}):\n",
    "        try:\n",
    "            hrefList.append(div.find('a',attrs={'target':'_blank'}).get('href'))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取论文的题目、机构、下载次数、被引次数时，也是从上面的html代码中获取信息，如题目为第一个a标签中的字符串内容；要找到机构，要先找到具有{'class':'year-count'}属性的span标签，再找到其中的第一个span标签，获得其中title属性的值；\n",
    "下载次数和被引次数为具有{'class':'count'}属性的span标签中的字符串内容。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaperInfo2(url,lst):\n",
    "    html=getHTMLText(url)\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    for div in soup.find_all('div',attrs={'class':'wz_content'}):\n",
    "        infoDict={}\n",
    "        try:\n",
    "            infoDict.update({'题目':div.find('a',attrs={'target':'_blank'}).text})\n",
    "            \n",
    "            span=div.find('span',attrs={'class':'year-count'})\n",
    "            infoDict.update({'发表机构':span.find('span').get('title')})\n",
    "            \n",
    "            for span in div.find_all('span',attrs={'class':'count'}):\n",
    "                infoDict.update({'下载数':span.string.split('|')[0].replace('下载次数（','').replace('）','')})\n",
    "                infoDict.update({'被引数':span.string.split('|')[1].replace('被引次数（','').replace('）','')})\n",
    "                \n",
    "            lst.append(infoDict)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在其中一篇论文的页面中，获取论文的题目、关键词时，可以选择从下面的html文段中获取信息。题目为title标签中的字符串内容，关键词为具有{'name':'keywords'}属性的meta标签中content属性的值。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<head>\n",
    "  <META http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
    "  <title>计算机视觉中摄像机定标综述--《自动化学报》2000年01期\n",
    "  </title>\n",
    "  <meta name=\"mobile-agent\" content=\"&#xA; format=html5;url=http://wap.cnki.net/touch/web/Journal/Article/MOTO200001006.html&#xA;          \">\n",
    "  <meta name=\"mobile-agent\" content=\"&#xA; format=xhtml; url=http://wap.cnki.net/qikan-MOTO200001006.html&#xA;          \">\n",
    "  <meta name=\"keywords\" content=\"计算机视觉 摄像机定标 摄影测量学 立体视觉 三维重建\">\n",
    "  <meta name=\"autoKeywords\" content=\"计算机视觉 摄像机定标 摄影测量学 立体视觉 三维重建\">\n",
    "  <meta name=\"description\" content=\"计算机视觉 摄像机定标 摄影测量学 立体视觉 三维重建\">\n",
    "  <link href=\"http://www.cnki.com.cn/cnki/CSS/detail.css?v=1130\" rel=\"stylesheet\" type=\"text/css\">\n",
    "  <link href=\"http://www.cnki.com.cn/cnki/CSS/header.css\" rel=\"stylesheet\" type=\"text/css\">\n",
    "  <link href=\"http://www.cnki.com.cn/cnki/CSS/footer.css\" rel=\"stylesheet\" type=\"text/css\">\n",
    "  <link href=\"http://www.cnki.com.cn/cnki/CSS/window.css\" rel=\"stylesheet\" type=\"text/css\">\n",
    "  <link href=\"http://www.cnki.com.cn/cnki/css/mycss.css\" rel=\"stylesheet\" type=\"text/css\">\n",
    "  <link href=\"http://www.cnki.com.cn/cnki/css/searchyuanjian.css\" rel=\"stylesheet\" type=\"text/css\">\n",
    "  <wxs src=\"http://www.cnki.com.cn/cnki/js/jquery-1.8.2.min.js\"></wxs>\n",
    "  <wxs src=\"http://www.cnki.com.cn/cnki/js/detail.js?v=1130\"></wxs>\n",
    "  <wxs src=\"http://www.cnki.com.cn/cnki/js/xgbook.js?v=1130\"></wxs>\n",
    "</head>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取作者姓名时，可以从下面的html代码段中获取信息。由于一篇论文的作者可能不止一人，因此可以用列表存储一篇文章的作者的姓名"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; width:740px; height:30px;\">\n",
    "  <a href=\"&#xA;                    http://yuanjian.cnki.com.cn/Search/Result?author=%E9%82%B1%E8%8C%82%E6%9E%97\" target=\"_blank\">邱茂林</a> &nbsp;\n",
    "  <a href=\"&#xA;                    http://yuanjian.cnki.com.cn/Search/Result?author=%E9%A9%AC%E9%A2%82%E5%BE%B7\" target=\"_blank\">马颂德</a> &nbsp;\n",
    "  <a href=\"&#xA;                    http://yuanjian.cnki.com.cn/Search/Result?author=%E6%9D%8E%E6%AF%85\" target=\"_blank\">李毅</a> &nbsp;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取机构和发表时间时，可以从下面的html代码中获取信息。其中b标签和font标枪均为在此页面的源码中第一次出现，因此可以用find()定位。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<b>\n",
    "  《自动化学报》\n",
    "</b>\n",
    "<font color=\"#0080ff\">2000年01期\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面给出获取文献的题目、作者、机构、发表时间、关键词的函数的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaperInfo(lst,hrefList):\n",
    "    for url in hrefList:\n",
    "        infoDict={}\n",
    "        try:\n",
    "            html=getHTMLText(url)\n",
    "            soup=BeautifulSoup(html,'html.parser')\n",
    "            \n",
    "            infoDict['题目']=soup.head.title.string.split('-')[0]\n",
    "            \n",
    "            infoDict['关键词']=soup.head.find_all('meta',attrs={'name':'keywords'})[0].get('content')\n",
    "            \n",
    "            authors=[]\n",
    "            for div in soup.find_all('div',style=\"text-align:center; width:740px; height:30px;\"):\n",
    "                for a in div.find_all('a',target=\"_blank\"):\n",
    "                    authors.append(a.string)\n",
    "            infoDict['作者']=authors\n",
    "            \n",
    "            infoDict['发表机构']=soup.find('b').string.strip().replace('《','').replace('》','')\n",
    "            \n",
    "            infoDict['发表时间']=soup.find('font').string.strip()\n",
    "            \n",
    "            lst.append(infoDict)\n",
    "            \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面给出完整的代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "\n",
    "#获得输入url的页面内容\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r=requests.get(url)\n",
    "        r.raise_for_status\n",
    "        r.encoding='utf-8'\n",
    "        return r.text\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "#获得在指定页数的查询页面中的查找到的论文的入口地址\n",
    "def getURLList(url,hrefList):\n",
    "    html=getHTMLText(url)\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    for div in soup.find_all('div',attrs={'class':'wz_content'}):\n",
    "        try:\n",
    "            hrefList.append(div.find('a',attrs={'target':'_blank'}).get('href'))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "#获得论文的被引数和下载数、题目、机构\n",
    "def getPaperInfo2(url,lst):\n",
    "    html=getHTMLText(url)\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    for div in soup.find_all('div',attrs={'class':'wz_content'}):\n",
    "        infoDict={}\n",
    "        try:\n",
    "            infoDict.update({'题目':div.find('a',attrs={'target':'_blank'}).text})\n",
    "            \n",
    "            span=div.find('span',attrs={'class':'year-count'})\n",
    "            infoDict.update({'发表机构':span.find('span').get('title')})\n",
    "            \n",
    "            for span in div.find_all('span',attrs={'class':'count'}):\n",
    "                infoDict.update({'下载数':span.string.split('|')[0].replace('下载次数（','').replace('）','')})\n",
    "                infoDict.update({'被引数':span.string.split('|')[1].replace('被引次数（','').replace('）','')})\n",
    "                \n",
    "            lst.append(infoDict)\n",
    "        except:\n",
    "            continue\n",
    "                      \n",
    "#获得论文的题目、作者、机构、时间、关键词\n",
    "def getPaperInfo(lst,hrefList):\n",
    "    for url in hrefList:\n",
    "        infoDict={}\n",
    "        try:\n",
    "            html=getHTMLText(url)\n",
    "            soup=BeautifulSoup(html,'html.parser')\n",
    "            \n",
    "            infoDict['题目']=soup.head.title.string.split('-')[0]\n",
    "            \n",
    "            infoDict['关键词']=soup.head.find_all('meta',attrs={'name':'keywords'})[0].get('content')\n",
    "            \n",
    "            authors=[]\n",
    "            for div in soup.find_all('div',style=\"text-align:center; width:740px; height:30px;\"):\n",
    "                for a in div.find_all('a',target=\"_blank\"):\n",
    "                    authors.append(a.string)\n",
    "            infoDict['作者']=authors\n",
    "            \n",
    "            infoDict['发表机构']=soup.find('b').string.strip().replace('《','').replace('》','')\n",
    "            \n",
    "            infoDict['发表时间']=soup.find('font').string.strip()\n",
    "            \n",
    "            lst.append(infoDict)            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "#用DataFrame把两个列表合并起来\n",
    "def getFormatDataset(lst1,lst2):\n",
    "    df1=pd.DataFrame(lst1)\n",
    "    df2=pd.DataFrame(lst2)\n",
    "    \n",
    "    df=pd.merge(df1,df2,on=['题目','发表机构'])\n",
    "    return df\n",
    "    \n",
    "def main():\n",
    "    quaryWord=input(\"请输入搜索信息:\")\n",
    "    pageNum=2\n",
    "    paperInfo=[]\n",
    "    paperInfo2=[]\n",
    "    start_url='http://search.cnki.com.cn'+'/Search.aspx?q='+quote(quaryWord)+'&rank=relevant=&cluster=all&val='\n",
    "    for i in range(pageNum):\n",
    "        hrefs=[]\n",
    "        quary_url=start_url+'&p='+str(i*15)\n",
    "        getURLList(quary_url,hrefs)\n",
    "        getPaperInfo2(quary_url,paperInfo2)\n",
    "        getPaperInfo(paperInfo,hrefs)\n",
    "\n",
    "    data=getFormatDataset(paperInfo,paperInfo2)\n",
    "    data.to_excel('E:/cnki.xlsx')    #把数据保存为excel文件\n",
    "    \n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
