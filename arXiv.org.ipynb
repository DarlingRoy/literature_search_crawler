{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv.org数据检索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arXiv.org是一个收录科学文献预印本的在线数据库，目前包含了超过50万篇文章，并且以每个月5000篇的速度增长着。目前，这个数据库包含：物理学，数学，计算机科学，定量生物学，量化金融、统计学、电子工程与系统科学几大分类。其最重要的特点就是“开放式获取”，每个人都可以免费地访问全文数据。如果想要看看某个领域的最新发展，知道大家都在干什么，找到自己感兴趣的研究领域最近发表的文章，使用arXiv.org数据库非常合适。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用arXiv.org，检索最近一周在某个领域上发表的文章。arXiv.org的领域可以分为上述的8大类，而物理学又可以细分为更多的领域，当然每个领域也可以包含若干个主题。因此，检索方式为根据用户选择的某个领域，获得这个领域最近一周内提交的文章信息，包含文章的题目、作者、主题、注释、期刊参考。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取所用的网页为：'https://arxiv.org/'。\n",
    "根据8个不同的邻域，分别定义8个不同的函数。下面先写出基本的代码框架："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#默认选择的区间为过去一周中所提交的论文。\n",
    "#按照arXiv上的分类，给出了8个学科领域，其中每个学科领域又可以细分为多个学科领域。\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#获得当前页面的HTML内容\n",
    "def getHTMLText(url):\n",
    "    return ''\n",
    "\n",
    "#获得文章的题目、作者、主题、注释、参考期刊信息\n",
    "def getPageInfo(url,lst):\n",
    "    pass\n",
    "\n",
    "#按照用户输入的检索的页面数，获得所有页码的URL\n",
    "def getURLList(keyword):\n",
    "    return ''\n",
    "\n",
    "#只有物理学下细分了有更多的领域\n",
    "def searchInPhysics(lst):\n",
    "    pass\n",
    "    \n",
    "def searchInMath(lst):\n",
    "    pass\n",
    "\n",
    "def searchInCs(lst):\n",
    "    pass\n",
    "\n",
    "def searchInQbio(lst):\n",
    "    pass\n",
    "\n",
    "def searchInQfin(lst):\n",
    "    pass\n",
    "\n",
    "def searchInStat(lst):\n",
    "    pass\n",
    "\n",
    "def searchInEess(lst):\n",
    "    pass\n",
    "\n",
    "def searchInEcon(lst):\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    start_url='https://arxiv.org/'\n",
    "    html=getHTMLText(start_url)\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    classes=[]  #用于存储8种不同领域类别的名称\n",
    "    \n",
    "    #爬取8个领域名称\n",
    "    select=soup.find('select',attrs={'name':'group'})\n",
    "    for option in select.find_all('option'):\n",
    "        classes.append(option.text.strip())\n",
    "    \n",
    "    #将8种领域类型名称打印出来    \n",
    "    for i in range(len(classes)):\n",
    "        print(str(i+1)+'、'+classes[i])\n",
    "        \n",
    "    choose=input(\"请选择检索的领域：\")\n",
    "    info=[]     #用于存储检索得到的论文的信息\n",
    "    \n",
    "    #按照用户选择的领域，在所选领域种进行检索\n",
    "    if choose==str(1):\n",
    "        searchInPhysics(info)\n",
    "    elif choose==str(2):\n",
    "        searchInMath(info)\n",
    "    elif choose==str(3):\n",
    "        searchInQbio(info)\n",
    "    elif choose==str(4):\n",
    "        searchInCs(info)\n",
    "    elif choose==str(5):\n",
    "        searchInQfin(info)\n",
    "    elif choose==str(6):\n",
    "        searchInStat(info)\n",
    "    elif choose==str(7):\n",
    "        searchInEess(info)\n",
    "    else:\n",
    "        searchInEcon(info)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以选择计算机科学领域为例，按每一页50条记录显示文章，可以看到网页的URL为'https://arxiv.org/list/cs/pastweek?skip=0&show=50'。\n",
    "其中cs为计算机科学的英文缩写，每个不同的领域按照英文缩写去标识，如数学为'math'，统计学为'stat'，等等；skip=0表示当前为第1页，skip=50表示第2页，skip=100表示第3页，以此类推；show=50表示每一页显示50条记录。所以searchInMath的函数定义为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchInCs(lst):\n",
    "    word='cs'\n",
    "    for url in getURLList(word):\n",
    "        getPageInfo(url,lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其余7个函数的定义与searchInCs类似，不作一一阐述。而物理学又可以细分为多个领域，因此在searchInPhysics函数中，可以再次获得用户的选择。再次在网页'https://arxiv.org/'\n",
    "上获得物理学中各个领域的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchInPhysics(lst):\n",
    "    start_url='https://arxiv.org/'\n",
    "    html=getHTMLText(start_url)\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    physicsClasses=[]   #用于存储所有更细一级的学科领域名称\n",
    "    abbrs=[]            #用于存储学科领域对应的英文缩写\n",
    "    word=''             #用于存储每个不同类别对应的英文缩写\n",
    "    \n",
    "    #找到所有物理学下的更细一级的学科领域的名称和英文缩写\n",
    "    for li in soup.find('ul').find_all('li'):\n",
    "        physicsClasses.append(li.find('a').text)\n",
    "        abbrs.append(li.find('a').get('href').split('/')[2])\n",
    "        \n",
    "    for i in range(len(physicsClasses)):\n",
    "        print(str(i+1)+'、'+physicsClasses[i])\n",
    "        \n",
    "    choose=input('请选择更细一级的检索领域：')\n",
    "    \n",
    "    word=abbrs[int(choose)-1]\n",
    "        \n",
    "    for url in getURLList(word):\n",
    "        getPageInfo(url,lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用getPageInfo函数获得文章的题目、作者、主题、注释、参考期刊信息。以搜索计算机科学领域为例，URL为'https://arxiv.org/list/cs/pastweek?skip=0&show=50'。\n",
    "每一条记录的题目、作者、主题、注释、参考期刊信息都可以定位在一个dd标签内，以获得第21条记录的文章的信息为例，其dd标签的内容为："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<dd>\n",
    " <div class=\"meta\">\n",
    "  <div class=\"list-title mathjax\">\n",
    "   <span class=\"descriptor\">\n",
    "    Title:\n",
    "   </span>\n",
    "   Towards Automated Infographic Design: Deep Learning-based  Auto-Extraction of Extensible Timeline\n",
    "  </div>\n",
    "  <div class=\"list-authors\">\n",
    "   <span class=\"descriptor\">\n",
    "    Authors:\n",
    "   </span>\n",
    "   <a href=\"/search/cs?searchtype=author&amp;query=Chen%2C+Z\">\n",
    "    Zhutian Chen\n",
    "   </a>\n",
    "   ,\n",
    "   <a href=\"/search/cs?searchtype=author&amp;query=Wang%2C+Y\">\n",
    "    Yun Wang\n",
    "   </a>\n",
    "   ,\n",
    "   <a href=\"/search/cs?searchtype=author&amp;query=Wang%2C+Q\">\n",
    "    Qianwen Wang\n",
    "   </a>\n",
    "   ,\n",
    "   <a href=\"/search/cs?searchtype=author&amp;query=Wang%2C+Y\">\n",
    "    Yong Wang\n",
    "   </a>\n",
    "   ,\n",
    "   <a href=\"/search/cs?searchtype=author&amp;query=Qu%2C+H\">\n",
    "    Huamin Qu\n",
    "   </a>\n",
    "  </div>\n",
    "  <div class=\"list-comments mathjax\">\n",
    "   <span class=\"descriptor\">\n",
    "    Comments:\n",
    "   </span>\n",
    "   10 pages, Automated Infographic Design, Deep Learning-based Approach, Timeline Infographics, Multi-task Model\n",
    "  </div>\n",
    "  <div class=\"list-journal-ref\">\n",
    "   <span class=\"descriptor\">\n",
    "    Journal-ref:\n",
    "   </span>\n",
    "   TVCG2019\n",
    "  </div>\n",
    "  <div class=\"list-subjects\">\n",
    "   <span class=\"descriptor\">\n",
    "    Subjects:\n",
    "   </span>\n",
    "   <span class=\"primary-subject\">\n",
    "    Human-Computer Interaction (cs.HC)\n",
    "   </span>\n",
    "   ; Graphics (cs.GR)\n",
    "  </div>\n",
    " </div>\n",
    "</dd>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，‘题目’为具有属性{'class':'list-title mathjax'}的div标签中的字符串内容；‘作者’为具有属性{'class':'list-authors'}的div标签中的字符串内容；‘主题’为具有属性{'class':'list-subjects'}的div标签中的字符串内容；‘注释’为具有属性{'class':'list-comments mathjax'}的div标签中的字符串内容；‘期刊参考’为具有属性{'class':'list-journal-ref'}的div标签中的字符串内容。注意的是，所有文章都有题目、作者和主题，但是有的文章没有注释或者期刊参考。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getPageInfo函数可以定义为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageInfo(url,lst):\n",
    "    html=getHTMLText(url)\n",
    "    if html=='':\n",
    "        return\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    for dd in soup.find_all('dd'):\n",
    "        infoDict={}\n",
    "        infoDict['题目']=dd.find('div',attrs={'class':'list-title mathjax'}).text.split(':')[1].strip()\n",
    "        infoDict['作者']=dd.find('div',attrs={'class':'list-authors'}).text.split(':')[1].strip().replace(',','').split('\\n')\n",
    "        infoDict['主题']=dd.find('div',attrs={'class':'list-subjects'}).text.split(':')[1].strip().split(';')\n",
    "        \n",
    "        if dd.find('div',attrs={'class':'list-comments mathjax'})==None:\n",
    "            pass\n",
    "        else:\n",
    "            infoDict['注释']=dd.find('div',attrs={'class':'list-comments mathjax'}).text.replace('Comments:','').strip()\n",
    "            \n",
    "        if dd.find('div',attrs={'class':'list-journal-ref'})==None:\n",
    "            pass\n",
    "        else:\n",
    "            infoDict['期刊参考']=dd.find('div',attrs={'class':'list-journal-ref'}).text.replace('Journal-ref:','').strip()\n",
    "            \n",
    "        lst.append(infoDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "最后，可以将得到的数据转化为DataFrame的格式，然后存储为一个excel文件，可用于后续的数据分析。下面给出完整的代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#默认选择的区间为过去一周中所提交的论文。\n",
    "#按照arXiv上的分类，给出了8个学科领域，其中每个学科领域又可以细分为多个学科领域。\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#获得当前页面的HTML内容\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r=requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        r.encoding='utf-8'\n",
    "        return r.text\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "#获得文章的题目、作者、主题、注释、参考期刊信息\n",
    "def getPageInfo(url,lst):\n",
    "    html=getHTMLText(url)\n",
    "    if html=='':\n",
    "        return\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    for dd in soup.find_all('dd'):\n",
    "        infoDict={}\n",
    "        infoDict['题目']=dd.find('div',attrs={'class':'list-title mathjax'}).text.split(':')[1].strip()\n",
    "        infoDict['作者']=dd.find('div',attrs={'class':'list-authors'}).text.split(':')[1].strip().replace(',','').split('\\n')\n",
    "        infoDict['主题']=dd.find('div',attrs={'class':'list-subjects'}).text.split(':')[1].strip().split(';')\n",
    "        \n",
    "        if dd.find('div',attrs={'class':'list-comments mathjax'})==None:\n",
    "            pass\n",
    "        else:\n",
    "            infoDict['注释']=dd.find('div',attrs={'class':'list-comments mathjax'}).text.replace('Comments:','').strip()\n",
    "            \n",
    "        if dd.find('div',attrs={'class':'list-journal-ref'})==None:\n",
    "            pass\n",
    "        else:\n",
    "            infoDict['期刊参考']=dd.find('div',attrs={'class':'list-journal-ref'}).text.replace('Journal-ref:','').strip()\n",
    "            \n",
    "        lst.append(infoDict)\n",
    "\n",
    "#按照用户输入的检索的页面数，获得所有页码的URL\n",
    "def getURLList(keyword):\n",
    "    pageNumber=input('请输入检索的总页数（按每一页50条记录）：')\n",
    "    lst=[]  #用于存储所有页码的URL\n",
    "    \n",
    "    #获得当前页码的URL\n",
    "    for i in range(int(pageNumber)):\n",
    "        lst.append('https://arxiv.org/list/'+keyword+'/pastweek?skip='+str(i*50)+'&show=50')\n",
    "    return lst\n",
    "\n",
    "#只有物理学下细分了有更多的领域\n",
    "def searchInPhysics(lst):\n",
    "    start_url='https://arxiv.org/'\n",
    "    html=getHTMLText(start_url)\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    physicsClasses=[]   #用于存储所有更细一级的学科领域名称\n",
    "    abbrs=[]            #用于存储学科领域对应的英文缩写\n",
    "    word=''             #用于存储每个不同类别对应的英文缩写\n",
    "    \n",
    "    #找到所有物理学下的更细一级的学科领域的名称和英文缩写\n",
    "    for li in soup.find('ul').find_all('li'):\n",
    "        physicsClasses.append(li.find('a').text)\n",
    "        abbrs.append(li.find('a').get('href').split('/')[2])\n",
    "        \n",
    "    for i in range(len(physicsClasses)):\n",
    "        print(str(i+1)+'、'+physicsClasses[i])\n",
    "        \n",
    "    choose=input('请选择更细一级的检索领域：')\n",
    "    \n",
    "    word=abbrs[int(choose)-1]\n",
    "        \n",
    "    for url in getURLList(word):\n",
    "        getPageInfo(url,lst)\n",
    "    \n",
    "def searchInMath(lst):\n",
    "    word='math'\n",
    "    for url in getURLList(word):\n",
    "        getPageInfo(url,lst)\n",
    "    \n",
    "\n",
    "def searchInCs(lst):\n",
    "    word='cs'\n",
    "    for url in getURLList(word):\n",
    "        getPageInfo(url,lst)\n",
    "\n",
    "def searchInQbio(lst):\n",
    "    word='q-bio'\n",
    "    for url in getURLList(word):\n",
    "        getPageInfo(url,lst)\n",
    "\n",
    "def searchInQfin(lst):\n",
    "    word='q-fin'\n",
    "    for url in getURLList(word):\n",
    "        getPageInfo(url,lst)\n",
    "\n",
    "def searchInStat(lst):\n",
    "    word='stat'\n",
    "    for url in getURLList(word):\n",
    "        getPageInfo(url,lst)\n",
    "\n",
    "def searchInEess(lst):\n",
    "    word='eess'\n",
    "    for url in getURLList(word):\n",
    "        getPageInfo(url,lst)\n",
    "\n",
    "def searchInEcon(lst):\n",
    "    word='econ'\n",
    "    for url in getURLList(word):\n",
    "        getPageInfo(url,lst)\n",
    "\n",
    "def main():\n",
    "    start_url='https://arxiv.org/'\n",
    "    html=getHTMLText(start_url)\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    classes=[]  #用于存储8种不同领域类别的名称\n",
    "    \n",
    "    #爬取8个领域名称\n",
    "    select=soup.find('select',attrs={'name':'group'})\n",
    "    for option in select.find_all('option'):\n",
    "        classes.append(option.text.strip())\n",
    "    \n",
    "    #将8种领域类型名称打印出来    \n",
    "    for i in range(len(classes)):\n",
    "        print(str(i+1)+'、'+classes[i])\n",
    "        \n",
    "    choose=input(\"请选择检索的领域：\")\n",
    "    info=[]     #用于存储检索得到的论文的信息\n",
    "    \n",
    "    #按照用户选择的领域，在所选领域种进行检索\n",
    "    if choose==str(1):\n",
    "        searchInPhysics(info)\n",
    "    elif choose==str(2):\n",
    "        searchInMath(info)\n",
    "    elif choose==str(3):\n",
    "        searchInQbio(info)\n",
    "    elif choose==str(4):\n",
    "        searchInCs(info)\n",
    "    elif choose==str(5):\n",
    "        searchInQfin(info)\n",
    "    elif choose==str(6):\n",
    "        searchInStat(info)\n",
    "    elif choose==str(7):\n",
    "        searchInEess(info)\n",
    "    else:\n",
    "        searchInEcon(info)\n",
    "        \n",
    "    data=pd.DataFrame(info)\n",
    "    data.to_excel('E:/arXiv.xlsx')\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
